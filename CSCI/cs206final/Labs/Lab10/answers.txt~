Name: Yifan Ge
Course: CSCI 206 / Prof. Perrone
Assignement: Lab10
Date: 11/08/2011

Table:

C-level   C-type   C-size   B-size    Set-assoc   Ms Rate   Mem-bandwidth
Problem-1
   1      unified    8k     4 bytes     1 bps     42.00%       1.7386	
   1      unified    8k     16 bytes    1 bps     16.78%       2.9351
   1      unified    16k    4 bytes     1 bps     32.72%       1.3733
   1      unified    16k    16 bytes    1 bps     13.50%       2.3940
   1      unified    32k    16 bytes    1 bps     10.64%       1.9284
   1      unified    64k    16 bytes    1 bps     09.20%       1.6905
   1      unified    128k   16 bytes    1 bps     08.41%       1.5514
   1      unified    256k   16 bytes    1 bps     01.99%       0.5090
   1      unified    512k   16 bytes    1 bps     01.99%       0.5090
Problem-2
   1      unified    8k     4 bytes     2 bps     22.90%       0.9864
   1      unified    8k     4 bytes     4 bps     22.86%       0.9851
   1      unified    128k   4 bytes     2 bps     22.82%       0.9835
   1      unified    128k   4 bytes     4 bps     22.82%       0.9835
Problem-3
   1      instr      8k     4 bytes     1 bps     01.07%       0.0427
   1      data       8k     4 bytes     1 bps     92.47%       3.8297

Problem 1

(1.1)
In the second row of the table, it shows that when the block size increased from 4 to 16 bytes the miss rate decreased from 42% to 16.78%. This huge reduction on the miss rate is due to the principal of spatial locality. However, the memory bandwidth increased from 1.7386 to 2.9351, which means we are getting more data from memory.

(1.2)
In the third row of the table, it shows that when the cache size increases, the miss rate decreases slightly. This is because that increasing the size of the cache increases the amount of data can be stored in the cache. This reduces the amount of RAM addresses need to store in each cache address. Unlike increasing the block size, increasing the Cache size decreases the memory bandwidth.

(1.3)
In the forth row of the table, it shows that when the cache size and the block size both increases, the miss rate decreases. But the memory bandwidth increases. Comparing the forth row and the second row, we found that the memory bandwidth decreased. This is because the increase of the cache size.

(1.4)
From the fifth row to the tenth row of the table shows the simulation result with the block size is 16 bytes (4 words) and increasing cache size from 16 bytes to 32 bytes. As the data indicates, the miss rate and mem-bandwidth decreases along with the increase of the cache size. However, after the size reaches 256k, the miss rate and mem-bandwidth seem to reach an asymtote and stablize themselves.


Problem 2

(2.1)
By increasing the associativity, we are increasing the number of blocks can be stored in each index of cache. So that we don't need to erase the data everytime when we need to retrieve another data from memory with the same cache index. 2-way set associativity does decrease the miss rate and improve the efficiency significantly. However, the 2-way and 4-way set associativity did have much differences on performance. My guess is the performance is limited by the block size. 

(2.2)
The increase of cache size did not do much improvement on the performance. This is because the small block size limited the improvement.

Problem 3
The 8k instruction cache is much more efficient than the 8k data cache. In the unified cache, the instruction part has more misses than the pure 8k instruction cache. But the data part has less misses than the pure 8k data cache. The unified cache nuturalizes the results from the pure data cache and instruction cache. 

Problem 4
From the 
